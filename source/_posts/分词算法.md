# æ–‡æœ¬é¢„å¤„ç†-åˆ†è¯ç®—æ³•

è¡Œæ–‡é€»è¾‘ï¼šåˆ†è¯ç®—æ³•=>Byte-Pair Encoding

**to work with a large amount of text data readily available on the internet, we need manipulation and cleaning of text which we commonly call text pre-processing in NLP.**[1]

å½“å‰tokenizationä¸»è¦åˆ†ä¸ºï¼šwordï¼Œsub-wordï¼Œ charlevel ä¸‰ä¸ªç±»å‹

ä¸»æµçš„sub-word tokenizationæ–¹æ³•æœ‰ï¼šWordPiece, Byte-Pair Encoding (BPE), Unigram, SentencePiece.

WordPieceæœ‰ä¸¤ç§ä»£ç å®ç°æ–¹å¼ï¼šbottom-up å’Œ top-downã€‚

# ä¸€ã€The Difference between Word, Subword and Character-Based Tokenization

è¿™ä¸ªéƒ¨åˆ†ä¼šå™è¿°ä¸€ä¸‹èƒŒæ™¯çŸ¥è¯†ï¼ŒåŒæ—¶ä»‹ç»ä¸åŒç²’åº¦çš„åˆ†è¯ç®—æ³•çš„å·®å¼‚ã€‚

**Tokenization** ç®€å•æ¥è¯´å°±æ˜¯å°†çŸ­è¯­ï¼Œå¥å­ï¼Œæ®µè½ï¼Œä¸€ä¸ªä¹ƒè‡³å¤šä¸ªæ–‡æœ¬åˆ‡åˆ†æˆæ›´å°çš„å•å…ƒã€‚è¿™äº›å°çš„å•å…ƒæˆä¸º**token**ã€‚æ‰€ä»¥è¯´è¿™äº›tokenå¯ä»¥æ˜¯wordï¼Œsubword,ç”šè‡³å¥å­ã€‚

Eg:

```
â€œLet us learn tokenization.â€

A word-based tokenization algorithm will break the sentence into words. The most common one is splitting based on space.

[â€œLetâ€, â€œusâ€, â€œlearnâ€, â€œtokenization.â€]

A subword-based tokenization algorithm will break the sentence into subwords.

[â€œLetâ€, â€œusâ€, â€œlearnâ€, â€œtokenâ€, â€œization.â€]

A character-based tokenization algorithm will break the sentence into characters.

[â€œLâ€, â€œeâ€, â€œtâ€, â€œuâ€, â€œsâ€, â€œlâ€, â€œeâ€, â€œaâ€, â€œrâ€, â€œnâ€, â€œtâ€, â€œoâ€, â€œkâ€, â€œeâ€, â€œnâ€, â€œiâ€, â€œzâ€, â€œaâ€, â€œtâ€, â€œiâ€, â€œoâ€, â€œnâ€, â€œ.â€]
```

These tokens are used to form the **vocabulary**, which is a set of unique tokens in a corpus (a dataset in NLP).

## 1.1 åŸºäºè¯çš„åˆ†è¯

å¯ä»¥é€šè¿‡è®¾ç½®è§„åˆ™ï¼ˆå¦‚ä»¥ç©ºæ ¼å’Œæ ‡ç‚¹ä½œä¸ºåˆ†éš”ç¬¦åˆ†è¯ï¼‰æ¥å®ç°ã€‚

**ç¼ºç‚¹**/**ç‰¹ç‚¹**ï¼š

- åœ¨å¤§çš„è¯­æ–™åº“ä¸Šä¼šå¯¼è‡´ç›¸å½“å¤§çš„vocabulary sizeã€‚
- ä¼šå°†åŒä¸€ä¸ªè¯çš„ä¸åŒå½¢å¼åœ¨è¯å…¸ä¸­å»ºç«‹ä¸åŒçš„æ ‡ç­¾ã€‚å¦‚â€œboysâ€ å’Œ â€œboyâ€ã€‚æ¨¡å‹ä¼šè®¤ä¸ºè¿™æ˜¯ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„è¯ã€‚
- é”™è¯¯çš„æ‹¼å†™ä¹Ÿä¼šå¯¼è‡´è¯æ±‡è¡¨å¤§å°çš„é¢å¤–å¢åŠ ã€‚

**è§£å†³æ–¹æ³•**ï¼š

- æ ¹æ®è¯é¢‘ï¼Œåªå½•å…¥ä¸€å®šæ•°é‡çš„è¯æ±‡ï¼Œå…¶ä»–çš„è¯ä¸€å¾‹æ ‡è®°æœªOOVï¼ˆOut Of Vocabularyï¼‰
  - æ¨¡å‹ä¼šå°†å¤§å¤šæ•°è¯è¯†åˆ«ä¸ºåŒä¸€ä¸ªOOV,è¿™æ˜¯ä¸€ä¸ªbig compromiseã€‚
  - â€œknowledgeâ€ çš„é”™è¯¯æ‹¼å†™â€œknowldgeâ€ ä¼šè¢«è¯†åˆ«ä¸ºOOVã€‚

## 1.2åŸºäºå­—ç¬¦ï¼ˆCharacterï¼‰çš„åˆ†è¯

**ä¼˜ç‚¹**ï¼š

- 256ä¸ªä¸åŒçš„å­—ç¬¦å°±å¯ä»¥è¡¨è¾¾è¶…è¿‡17wä¸ªå•è¯ï¼Œä¹Ÿå‡ ä¹ä¸ä¼šæœ‰OOVçš„é—®é¢˜ã€‚
- é”™è¯¯çš„æ‹¼å†™ä¹Ÿä¼šè¢«è€ƒè™‘ï¼Œè€Œä¸æ˜¯ä»…ä»…è¢«æ ‡è®°æˆOOVå¹¶æŸå¤±ä¿¡æ¯ã€‚

**ç¼ºç‚¹**ï¼š

- å­—ç¬¦æ‰¿è½½çš„ä¿¡æ¯é‡ç›¸è¾ƒäºwordå¤ªå°‘äº†ï¼ˆfor Englishï¼‰ã€‚
- äº§ç”Ÿçš„tokenæ•°é‡å¤ªå¤šäº†ï¼Œä¸€ä¸ªâ€œknowledgeâ€å°±ä¼šäº§ç”Ÿ9ä¸ªtokenã€‚

## 1.3 Subword-based tokenization 

Another popular tokenization is subword-based tokenization which is a solution between word and character-based tokenization.

åŸºäºå­—è¯çš„åˆ†è¯ç®—æ³•æœ‰ä¸¤ä¸ªåŸåˆ™ï¼š

â€‹	1.ä¸å°†é«˜é¢‘ç‡çš„è¯æ±‡åˆ‡åˆ†ä¸ºæ›´å°çš„å­—è¯

â€‹	2.å°†ä½é¢‘è¯åˆ‡åˆ†ä¸º**å°çš„** **æ›´æœ‰æ„ä¹‰çš„**subwords.

---

The subword-based tokenization algorithms generally use a special symbol to indicate which word is the start of the token and which word is the completion of the start of the token. For example, â€œtokenizationâ€ can be split into â€œtokenâ€ and â€œ##izationâ€ which indicates that â€œtokenâ€ is the start of the word and â€œ##izationâ€ is the completion of the word.



A few common subword-based tokenization algorithms are [WordPiece](https://arxiv.org/pdf/1609.08144v2.pdf) used by [BERT](https://arxiv.org/pdf/1810.04805.pdf) and [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf), [Unigram](https://arxiv.org/pdf/1804.10959.pdf) by [XLNet](https://arxiv.org/pdf/1906.08237.pdf) and [ALBERT](https://arxiv.org/pdf/1909.11942.pdf), and [Bye-Pair Encoding](https://arxiv.org/pdf/1508.07909.pdf) by [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf). ğŸ˜Š

---

æåˆ°äº†å‡ ä¸ªå­è¯åˆ†è¯ç®—æ³•ï¼ŒBERTä½¿ç”¨çš„WordPiece,XLNetä½¿ç”¨çš„Unigramï¼ŒGPT-2å’ŒRoBERTaä½¿ç”¨çš„Bye-Pair Encoding.

## 2.WordPiece



##  3.BPE (Byte-Pair Encoding)

[Byte-Pair Encoding: Subword-based tokenization | Towards Data Science](https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0)

ç®€å•æè¿°ï¼š

æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„æ½œåœ¨åˆå¹¶å¯èƒ½ï¼Œç„¶ååˆå¹¶ï¼Œéšåæ›´æ–°Token-Frequence Tableï¼›é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°è¿­ä»£æ¬¡æ•°åˆ°ä¸€å®šç¨‹åº¦orè¯è¡¨å¤§å°è¾¾åˆ°ä¸€å®šç¨‹åº¦ã€‚

è¿™ä¸ªè¿‡ç¨‹æ˜¯è´ªå¿ƒçš„ï¼Œä¹Ÿæ˜¯æœ‰æ•ˆçš„ã€‚

## WordPiece



## äºŒã€åµŒå…¥å±‚

ç›®å‰ä¸»æµåµŒå…¥æ¨¡å‹ç”Ÿæˆæ–‡æœ¬åµŒå…¥çš„å¸¸è§æ–¹æ³•ï¼š

- â€œå– [CLS]â€ï¼šä»¥åŸºäº Transformer æ¶æ„çš„æ¨¡å‹ä¸ºå…¸å‹ä»£è¡¨ï¼Œé€šè¿‡æå–ç‰¹å®šæ ‡è¯†ï¼ˆå¦‚ [CLS]ï¼‰å¯¹åº”çš„éšè—çŠ¶æ€å‘é‡æ¥è·å–æ–‡æœ¬åµŒå…¥ï¼Œæ•æ‰æ–‡æœ¬æ•´ä½“è¯­ä¹‰ã€‚
- â€œå¹³å‡æ± åŒ– /max æ± åŒ–â€ï¼šå¯¹è¾“å…¥æ–‡æœ¬çš„æ‰€æœ‰ token éšè—çŠ¶æ€è¿›è¡Œå¹³å‡æˆ–å–æœ€å¤§å€¼æ“ä½œï¼Œç®€å•ç›´è§‚åœ°ä»æ•´ä½“ç‰¹å¾æˆ–çªå‡ºç‰¹å¾çš„è§’åº¦ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚
- â€œå–ç‰¹æ®Šç»“æ„ï¼ˆRNN åŠå…¶å˜ä½“çš„æœ€åæ—¶é—´æ­¥ã€åŸºäºæ³¨æ„åŠ›çš„åŠ æƒæ±‚å’Œï¼‰â€ï¼šé’ˆå¯¹ RNN åŠå…¶å˜ä½“ï¼Œåˆ©ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥éšè—çŠ¶æ€æºå¸¦çš„åºåˆ—ä¿¡æ¯ä½œä¸ºæ–‡æœ¬åµŒå…¥ï¼›åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åŠ æƒæ±‚å’Œåˆ™æ˜¯é€šè¿‡è®¡ç®— token æƒé‡è¿›è¡ŒåŠ æƒæ±‚å’Œæ¥ç”Ÿæˆæ›´å…·ä»£è¡¨æ€§çš„æ–‡æœ¬åµŒå…¥ï¼Œå¼ºè°ƒå¯¹é‡è¦ä¿¡æ¯çš„å…³æ³¨ ã€‚

# å‚è€ƒ

\[1][Word, Subword and Character-based tokenization: Know the difference | Towards Data Science](https://towardsdatascience.com/word-subword-and-character-based-tokenization-know-the-difference-ea0976b64e17)