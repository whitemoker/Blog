# 文本预处理-分词算法

行文逻辑：分词算法=>Byte-Pair Encoding

**to work with a large amount of text data readily available on the internet, we need manipulation and cleaning of text which we commonly call text pre-processing in NLP.**[1]

当前tokenization主要分为：word，sub-word， charlevel 三个类型

主流的sub-word tokenization方法有：WordPiece, Byte-Pair Encoding (BPE), Unigram, SentencePiece.

WordPiece有两种代码实现方式：bottom-up 和 top-down。

# 一、The Difference between Word, Subword and Character-Based Tokenization

这个部分会叙述一下背景知识，同时介绍不同粒度的分词算法的差异。

**Tokenization** 简单来说就是将短语，句子，段落，一个乃至多个文本切分成更小的单元。这些小的单元成为**token**。所以说这些token可以是word，subword,甚至句子。

Eg:

```
“Let us learn tokenization.”

A word-based tokenization algorithm will break the sentence into words. The most common one is splitting based on space.

[“Let”, “us”, “learn”, “tokenization.”]

A subword-based tokenization algorithm will break the sentence into subwords.

[“Let”, “us”, “learn”, “token”, “ization.”]

A character-based tokenization algorithm will break the sentence into characters.

[“L”, “e”, “t”, “u”, “s”, “l”, “e”, “a”, “r”, “n”, “t”, “o”, “k”, “e”, “n”, “i”, “z”, “a”, “t”, “i”, “o”, “n”, “.”]
```

These tokens are used to form the **vocabulary**, which is a set of unique tokens in a corpus (a dataset in NLP).

## 1.1 基于词的分词

可以通过设置规则（如以空格和标点作为分隔符分词）来实现。

**缺点**/**特点**：

- 在大的语料库上会导致相当大的vocabulary size。
- 会将同一个词的不同形式在词典中建立不同的标签。如“boys” 和 “boy”。模型会认为这是两个完全不同的词。
- 错误的拼写也会导致词汇表大小的额外增加。

**解决方法**：

- 根据词频，只录入一定数量的词汇，其他的词一律标记未OOV（Out Of Vocabulary）
  - 模型会将大多数词识别为同一个OOV,这是一个big compromise。
  - “knowledge” 的错误拼写“knowldge” 会被识别为OOV。

## 1.2基于字符（Character）的分词

**优点**：

- 256个不同的字符就可以表达超过17w个单词，也几乎不会有OOV的问题。
- 错误的拼写也会被考虑，而不是仅仅被标记成OOV并损失信息。

**缺点**：

- 字符承载的信息量相较于word太少了（for English）。
- 产生的token数量太多了，一个“knowledge”就会产生9个token。

## 1.3 Subword-based tokenization 

Another popular tokenization is subword-based tokenization which is a solution between word and character-based tokenization.

基于字词的分词算法有两个原则：

​	1.不将高频率的词汇切分为更小的字词

​	2.将低频词切分为**小的** **更有意义的**subwords.

---

The subword-based tokenization algorithms generally use a special symbol to indicate which word is the start of the token and which word is the completion of the start of the token. For example, “tokenization” can be split into “token” and “##ization” which indicates that “token” is the start of the word and “##ization” is the completion of the word.



A few common subword-based tokenization algorithms are [WordPiece](https://arxiv.org/pdf/1609.08144v2.pdf) used by [BERT](https://arxiv.org/pdf/1810.04805.pdf) and [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf), [Unigram](https://arxiv.org/pdf/1804.10959.pdf) by [XLNet](https://arxiv.org/pdf/1906.08237.pdf) and [ALBERT](https://arxiv.org/pdf/1909.11942.pdf), and [Bye-Pair Encoding](https://arxiv.org/pdf/1508.07909.pdf) by [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf). 😊

---

提到了几个子词分词算法，BERT使用的WordPiece,XLNet使用的Unigram，GPT-2和RoBERTa使用的Bye-Pair Encoding.

## 2.WordPiece



##  3.BPE (Byte-Pair Encoding)

[Byte-Pair Encoding: Subword-based tokenization | Towards Data Science](https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0)

简单描述：

找到频率最高的潜在合并可能，然后合并，随后更新Token-Frequence Table；重复这个过程，直到迭代次数到一定程度or词表大小达到一定程度。

这个过程是贪心的，也是有效的。

## WordPiece



## 二、嵌入层

目前主流嵌入模型生成文本嵌入的常见方法：

- “取 [CLS]”：以基于 Transformer 架构的模型为典型代表，通过提取特定标识（如 [CLS]）对应的隐藏状态向量来获取文本嵌入，捕捉文本整体语义。
- “平均池化 /max 池化”：对输入文本的所有 token 隐藏状态进行平均或取最大值操作，简单直观地从整体特征或突出特征的角度生成文本嵌入。
- “取特殊结构（RNN 及其变体的最后时间步、基于注意力的加权求和）”：针对 RNN 及其变体，利用最后一个时间步隐藏状态携带的序列信息作为文本嵌入；基于注意力机制的加权求和则是通过计算 token 权重进行加权求和来生成更具代表性的文本嵌入，强调对重要信息的关注 。

# 参考

\[1][Word, Subword and Character-based tokenization: Know the difference | Towards Data Science](https://towardsdatascience.com/word-subword-and-character-based-tokenization-know-the-difference-ea0976b64e17)